docker-compose down -v
- Stops all containers in the compose project.
- Removes the containers, networks, and volumes (-v) created by Docker Compose.
- Use this to completely reset the environment (including HDFS data).

docker-compose up -d --build
- Builds Docker images from your Dockerfiles (--build).
- Starts containers in detached mode (-d) so they run in the background.
- Creates the network and volumes if they don’t exist.

docker ps -a
- Lists all Docker containers, including running and stopped ones.
- Useful to check container status, IDs, and ports.

docker network ls
- Lists all Docker networks.
- Helps verify that your hadoop network exists and is being used by containers.

docker exec -it spark-client bash
- Opens an interactive shell (-it bash) inside the spark-client container.
- Allows you to run commands inside the container, e.g., Spark commands, Hadoop
  commands, or Python scripts.

/opt/spark# spark-submit --master spark://spark-master:7077 /scripts/test_pyspark.py
- Submits a PySpark job to your Spark cluster.
- --master spark://spark-master:7077 tells Spark where the Master is running.
- /scripts/test_pyspark.py is the Python script you want to run on the cluster.
  Output and computation happen across Spark Master and Worker nodes.

/opt/spark#  pyspark --master spark://spark-master:7077
- Open interactive spark session.

docker exec -i namenode hdfs dfs -ls -h /user/data
- Check uploaded files directory

docker exec spark-client spark-submit /scripts/clean_and_transform_data.py
- run cleaning script with pyspark environment
- spark-submit is the official launcher for Spark jobs.It automatically:
- starts a JVM with all Spark and Hadoop JARs on the classpath,
- configures Py4J to connect Python ↔ Java,
- passes cluster config (Spark master, YARN, etc.),
- and runs your script inside a full Spark context.


docker compose up --scale spark-worker=2
- run docker containers with two spark workers