services:
  namenode:
    build: ./docker/hadoop
    container_name: namenode
    environment:
      - HADOOP_ROLE=namenode
    ports:
      - "9870:9870"
      - "9000:9000"
    volumes:
      - namenode_data:/hadoop/dfs/name
    networks:
      - hadoop

  datanode:
    build: ./docker/hadoop
    container_name: datanode
    environment:
      - HADOOP_ROLE=datanode
    depends_on:
      - namenode
    ports:
      - "9864:9864"
    volumes:
      - datanode_data:/hadoop/dfs/data
    networks:
      - hadoop

  spark-master:
    build: ./docker/spark
    container_name: spark-master
    command: /opt/spark/bin/spark-class org.apache.spark.deploy.master.Master
    ports:
      - "8080:8080"
      - "7077:7077"
    networks:
      - hadoop

  spark-worker:
    build: ./docker/spark
    command: /opt/spark/bin/spark-class org.apache.spark.deploy.worker.Worker spark://spark-master:7077
    depends_on:
      - spark-master
    networks:
      - hadoop
    deploy:
      replicas: 2

  spark-client:
    build: ./docker/spark
    container_name: spark-client
    depends_on:
      - spark-master
      - namenode
      - datanode
    volumes:
      - ./scripts:/scripts
    networks:
      - hadoop
    tty: true
    stdin_open: true

volumes:
  namenode_data:
  datanode_data:

networks:
  hadoop:
    driver: bridge
