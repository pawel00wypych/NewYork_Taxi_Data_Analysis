version: "3"

services:
  namenode:
    build: ./docker/hadoop
    container_name: namenode
    command: bash -c "hdfs namenode -format && hdfs namenode"
    ports:
      - "9870:9870"
      - "9000:9000"
    volumes:
      - namenode_data:/hadoop/dfs/name
    networks:
      - hadoop

  datanode:
    build: ./docker/hadoop
    container_name: datanode
    command: hdfs datanode
    depends_on:
      - namenode
    ports:
      - "9864:9864"
    volumes:
      - datanode_data:/hadoop/dfs/data
    networks:
      - hadoop

  spark-master:
    build: ./docker/spark
    command: /opt/spark/bin/spark-class org.apache.spark.deploy.master.Master
    ports:
      - "8080:8080"
      - "7077:7077"
    networks:
      - hadoop

  spark-worker:
    build: ./docker/spark
    command: /opt/spark/bin/spark-class org.apache.spark.deploy.worker.Worker spark://spark-master:7077
    depends_on:
      - spark-master
    ports:
      - "8081:8081"
    networks:
      - hadoop

  spark-client:
    build: ./docker/spark
    depends_on:
      - spark-master
      - namenode
    volumes:
      - ./scripts:/scripts
    networks:
      - hadoop
    tty: true
    stdin_open: true

volumes:
  namenode_data:
  datanode_data:

networks:
  hadoop:
    driver: bridge
